{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4024bf12-53c5-4f8e-8994-a5b0073cdb08",
   "metadata": {},
   "source": [
    "This is where the magic happens:\n",
    "\n",
    "# MODELLING\n",
    "\n",
    "In this notebook I'll work on a model to what a tourist will spend when vacationing in Tanzania.\n",
    "The evaluation metric for the model is **Mean Absolute Error**.\n",
    "\n",
    "\n",
    "To do:\n",
    "- check if variables used in model are correct\n",
    "- add comments to so far modelling steps\n",
    "- feature selection/feature engineering\n",
    "- check again outlier handling\n",
    "- hyperparameter tuning\n",
    "- fix overfitting\n",
    "- error analysis\n",
    "- interpretation/visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6b52b5-7a23-477c-af5a-c0a60112e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some packages that I'll need\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# set color scheme\n",
    "cpal = [\"#f94144\",\"#f3722c\",\"#f8961e\",\"#f9844a\",\"#f9c74f\",\"#90be6d\",\"#43aa8b\",\"#4d908e\",\"#577590\",\"#277da1\"]\n",
    "\n",
    "# seaborn theme\n",
    "sns.set()\n",
    "\n",
    "# use natural numbers\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "\n",
    "# set random seed\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7607d78f-5887-42b8-87ea-f75ffeb7473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "TZA = pd.read_csv('data/Train.csv')\n",
    "\n",
    "# load subregions\n",
    "subregions = pd.read_csv('data/subregions.csv')\n",
    "subregions.drop(['Unnamed: 0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b7ecb-d705-4303-a9e5-e7bbae09acb0",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "I'm going to split the train and test data now, very in the beginning to avoid data leakage.\n",
    "\n",
    "I'm not using the sklearn train test split as I had concerning results in the first run (much better performance on test than on train data, I assume the different values of the target variable weren't well splitted). That's why I use Verstack stratified continuos split which allows me to stratify by continuos target variable. (It makes sure that different bins of total_cost are evenly divided among train and test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c20b6d6-24bc-47b0-b8d4-b1d7ad0a1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train, test = scsplit(TZA, stratify = TZA['total_cost'], test_size = 0.3, random_state = RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee23e65-64a9-4f27-9cd5-a9bf3c002d56",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "I am going to preprocess the data now. I'll do it separately for train and test data. I start with the very basics for my Baseline Model.\n",
    "\n",
    "#### Missing values and minor adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c6478a-3918-4b32-857b-83a4fc61c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to handle missing data and to make some basic adjustments on the dataset\n",
    "\n",
    "def basic_preprocessing_baseline(df):\n",
    "    # fill NaN total_male/total_female with 0\n",
    "    df['total_male'] = df['total_male'].fillna(0)\n",
    "    df['total_female'] = df['total_female'].fillna(0)\n",
    "    \n",
    "    # fill NaN travel_with with \"Alone\" if total_male plus total_female is one\n",
    "    df.loc[df['total_female'] + df['total_male'] == 1, 'travel_with'] = 'Alone'\n",
    "    \n",
    "    # fill remaining NaN travel_with with missing\n",
    "    df['travel_with'] = df['travel_with'].fillna('missing')\n",
    "    \n",
    "    # fill NaN most_impressing with \"No comments\"\n",
    "    df['most_impressing'] = df['most_impressing'].fillna('No comments')\n",
    "   \n",
    "    # drop id column\n",
    "    df = df.drop(['ID'], axis =1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4daee7b0-2ab1-4178-9bfa-5304f338d6bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply function on train data\n",
    "train_bl = basic_preprocessing_baseline(train)\n",
    "# apply function on test data\n",
    "test_bl = basic_preprocessing_baseline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc87ef02-6700-458f-a8fc-f8a585349e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate target variable, both in train and test data\n",
    "\n",
    "X_train_bl = train_bl.drop(['total_cost'], axis=1)\n",
    "y_train_bl = train_bl['total_cost']\n",
    "\n",
    "X_test_bl = test_bl.drop(['total_cost'], axis=1)\n",
    "y_test_bl = test_bl['total_cost']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0062dea-f1dd-4fd5-9de6-f9708c8b59d3",
   "metadata": {},
   "source": [
    "### Build Pipelines\n",
    "\n",
    "I'm going to build some pipelines now. They'll make modelling easier and faster.\n",
    "\n",
    "I start with a pipeline for the categorical features. I use a One Hot Encoder to convert them into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98500513-dc9d-4e66-8189-c6250bee8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of categorical features\n",
    "cat_features_bl = list(X_train_bl.columns[X_train_bl.dtypes==object])\n",
    "\n",
    "# build pipeline\n",
    "cat_pipeline_bl = Pipeline([\n",
    "    ('1hot', OneHotEncoder(handle_unknown= 'ignore', drop = 'first'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d64a46-f56c-4fc0-a124-fba6edf79f72",
   "metadata": {},
   "source": [
    "For the numerical features I'll use a Robust Scaler. It can handle outliers pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf948b9a-bedd-4f8b-8448-35c457319201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of numerical features\n",
    "num_features_bl = list(X_train_bl.columns[X_train_bl.dtypes!=object])\n",
    "\n",
    "# build pipeline\n",
    "num_pipeline_bl = Pipeline([\n",
    "    ('rob_scaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba64739-0e4a-43d5-8351-f66de99d4e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both pipelines in a preprocessor\n",
    "preprocessor_bl = ColumnTransformer([\n",
    "    ('num', num_pipeline_bl, num_features_bl),\n",
    "    ('cat', cat_pipeline_bl, cat_features_bl)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114112e0-67a1-440f-80f6-15d29c4f3081",
   "metadata": {},
   "source": [
    "### Baseline Model\n",
    "\n",
    "First I'm going to train a linear regression model. Except for some NaN imputations and the basic preprocessing we haven't made adjustments on the data yet. The result of the Baseline Model will serve me as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70f400d5-ac51-418a-b52b-0775ca49d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline that combines the preprocessor and the linear regression model\n",
    "pipe_linreg_bl = Pipeline([\n",
    "    ('preprocessor', preprocessor_bl),\n",
    "    ('linreg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e508a439-dd51-4b55-a34b-5394430fe6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error Baseline Model (train data): 5858302.30\n"
     ]
    }
   ],
   "source": [
    "# cross validate to check how the model performs on the train data\n",
    "y_train_predicted_bl_cv = cross_val_predict(pipe_linreg_bl, X_train_bl, y_train_bl, cv=100)\n",
    "\n",
    "# print MAE of Baseline Model (train data)\n",
    "print(\"Mean Absolute Error Baseline Model (train data): {:.2f}\".format(mean_absolute_error(y_train_bl, y_train_predicted_bl_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ec3dec-8819-4f1c-a19e-ee38213cee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error Baseline Model (test data): 5895242.84\n"
     ]
    }
   ],
   "source": [
    "# fit the actual model\n",
    "y_train_predicted_bl = pipe_linreg_bl.fit(X_train_bl, y_train_bl)\n",
    "\n",
    "# make predictions for the test data\n",
    "y_test_predicted_bl = pipe_linreg_bl.predict(X_test_bl)\n",
    "# print MAE of Baseline Model (test data)\n",
    "print(\"Mean Absolute Error Baseline Model (test data): {:.2f}\".format(mean_absolute_error(y_test_bl, y_test_predicted_bl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db632f4b-23dc-4ae5-8114-13f0a8574426",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "The Mean Absolute Error of the Baseline Model is 5895242.84 Tanzanian Schillig TZS.\n",
    "What does that mean?\n",
    "The MAE is the sum of absolute errors divided by the sample size $n$ where $y_i$ is the prediction and $x_i$ is the true value:\n",
    "\n",
    "$$ \n",
    "MAE = \\frac {\\sum_{i=1}^n \\vert y_i - x_i \\vert} {n}\n",
    "$$\n",
    "\n",
    "The MAE uses the same scale as the data, so in this case TZS. \n",
    "\n",
    "So, on average, the model's predictions are 5895242.84 TZS off the true value. This is roughly 2156 Euro and seems quite a lot. \n",
    "\n",
    "Our aim is to improve (lower) this metric as much as we can.\n",
    "\n",
    "So let's start with the actual \n",
    "\n",
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8305f0fe-0669-4694-8fcd-9ea809fc95d6",
   "metadata": {},
   "source": [
    "First I'll make some more adjustments on the dataset - some of them we already saw in the EDA. I want to reduce noise from the dataset and add some more information which seems useful to me. I need a function that I can apply to both the train and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad02c9e8-d17a-4973-aba7-9bde9bf94bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to handle missing data and to make some adjustments in the data set\n",
    "\n",
    "def adjustments1(df):\n",
    "    # fill NaN total_male/total_female with 0\n",
    "    df['total_male'] = df['total_male'].fillna(0)\n",
    "    df['total_female'] = df['total_female'].fillna(0)\n",
    "    \n",
    "    # add a column group_size based on total_male/total_female\n",
    "    df['group_size'] = df['total_female'] + df['total_male']\n",
    "    \n",
    "    # fill NaN travel_with with \"Alone\" if group_size is one\n",
    "    df.loc[df.group_size == 1, 'travel_with'] = 'Alone'\n",
    "    \n",
    "    # fill remaining NaN travel_with with missing\n",
    "    df['travel_with'] = df['travel_with'].fillna('missing')\n",
    "    \n",
    "    # fill NaN most_impressing with \"No comments\"\n",
    "    df['most_impressing'] = df['most_impressing'].fillna('No comments')\n",
    "    \n",
    "    # add a column total_nights based on night_zanzibar/night_mainland\n",
    "    df['total_nights'] = df['night_zanzibar'] + df['night_mainland']\n",
    "    \n",
    "    # handle group_size equals zero: either replace by 1 if alone traveller or median group size of the train data\n",
    "    df.loc[(df.group_size == 0) & (df.travel_with == 'Alone'), 'group_size'] = 1\n",
    "    df.loc[df.group_size == 0, 'group_size'] = train['group_size'].median()\n",
    "\n",
    "    # handle total_nights equals zero: replace by median total_nights of the train data\n",
    "    df.loc[df.total_nights == 0, 'total_nights'] = train['total_nights'].median()\n",
    "\n",
    "    # drop id column\n",
    "    df = df.drop(['ID'], axis =1)\n",
    "    \n",
    "    # drop night_mainland column (to avoid multicollinearity)\n",
    "    df = df.drop(['night_mainland'], axis =1)\n",
    "    \n",
    "    # drop total_male column (to avoid multicollinearity)\n",
    "    df = df.drop(['total_male'], axis =1)\n",
    "\n",
    "    # add subregions just as in the EDA\n",
    "    df['country'] = df['country'].str.lower()\n",
    "    df = df.replace({'country' : {'united states of america': 'united States', 'swaziland' : 'eswatini', 'cape verde' : 'cabo verde', 'swizerland' : 'switzerland', 'ukrain' : 'ukraine','malt' : 'malta', 'burgaria' : 'bulgaria', 'korea' : 'south korea', 'comoro' : 'comoros', 'scotland' : 'united kingdom', 'russia' : 'russia', 'srilanka': 'sri lanka'}})\n",
    "    df = df.replace({'country' : {'ivory coast': \"côte d'ivoire\", 'drc' : 'congo', 'uae' : 'united arab emirates', 'trinidad tobacco' : 'trinidad and tobago', 'costarica' : 'costa rica', 'philipines' : 'philippines', 'djibout' : 'djibouti', 'morroco' : 'morocco'}})\n",
    "    df['country'] = df['country'].str.capitalize()\n",
    "    df = pd.merge(df, subregions, how ='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83fe6e40-0771-4a0c-824e-c1566abbfdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to train and test data\n",
    "train_model = adjustments1(train)\n",
    "test_model = adjustments1(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85127daa-8f35-4f6c-b9de-d6cc05d2a7bd",
   "metadata": {},
   "source": [
    "We have some outliers in the numeric columns, including our target variable. I want to get rid of them and try (for the first time) an outlier detection algorithm:\n",
    "\n",
    "### Outlier handling: Isolation Forest\n",
    "\n",
    "In this first iteration I'll use the Isolation Forest to detect anomalies and I'll then delete these observations. (Possibly I'll handle this differently in another iteration.) It is important that we apply this method to the train data only. Outliers in the test data remain untouched.\n",
    "The Isolation Forest will not instantly delete the columns, but give a score for each row: either -1 (anomaly) or 1 (normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e4947e-8431-4f9a-a623-b31d996ab8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_female</th>\n",
       "      <th>night_zanzibar</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>group_size</th>\n",
       "      <th>total_nights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4233255.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>52377000.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>7458750.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>828750.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2836662.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16112243.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3362</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3987945.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>314925.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1560000.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3365</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41233726.10</td>\n",
       "      <td>2.00</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3366 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_female  night_zanzibar  total_cost  group_size  total_nights\n",
       "0             1.00            0.00  4233255.00        1.00          3.00\n",
       "1             3.00            4.00 52377000.00        5.00         10.00\n",
       "2             0.00            4.00  7458750.00        3.00         15.00\n",
       "3             2.00            3.00   828750.00        3.00          3.00\n",
       "4             1.00            0.00  2836662.50        2.00          7.00\n",
       "...            ...             ...         ...         ...           ...\n",
       "3361          2.00            0.00 16112243.00        4.00          6.00\n",
       "3362          1.00            0.00  3987945.00        1.00         12.00\n",
       "3363          0.00            0.00   314925.00        1.00          3.00\n",
       "3364          0.00            0.00  1560000.00        1.00          4.00\n",
       "3365          1.00            0.00 41233726.10        2.00         15.00\n",
       "\n",
       "[3366 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe with only numeric columns\n",
    "iso_features = train_model[['total_female', 'night_zanzibar', 'total_cost', 'group_size',\n",
    "       'total_nights']]\n",
    "\n",
    "# have a look\n",
    "iso_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea0c377f-f74a-4f2d-9ce9-b5f5d34b08c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3366, 23)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size of this data set\n",
    "train_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db9db160-01a0-4239-a9f6-101bf1874746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define isolation forest\n",
    "isolation_forest = IsolationForest()\n",
    "\n",
    "# fit isolation forest\n",
    "isolation_forest.fit(iso_features)\n",
    "\n",
    "# make predictions\n",
    "predictions = isolation_forest.predict(iso_features)\n",
    "\n",
    "# create data frame with Isolation Forest Scores\n",
    "predictions_df = pd.DataFrame(predictions, columns = ['Iso_Score'])\n",
    "\n",
    "# merge this data frame into the train_model data frame\n",
    "train_model = pd.concat([train_model, predictions_df], axis=1)\n",
    "\n",
    "# keep only those with Iso_Score is 1 (normal)\n",
    "train_model = train_model[train_model['Iso_Score'] == 1]\n",
    "\n",
    "# delete the Iso_Score column\n",
    "train_model = train_model.drop(['Iso_Score'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2408a71-217e-4159-89e8-aca50e7df431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2936, 23)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size of data set without anomalies\n",
    "train_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123cd96-565c-4504-b398-e837b875371a",
   "metadata": {},
   "source": [
    "430 observations were deleted. We might find a better way for the outlier handling later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03337af0-3d4c-41da-b7c1-4f3d52c2e716",
   "metadata": {},
   "source": [
    "I'm now going to use a \n",
    "\n",
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e55a50b-3199-4f81-90ba-41b3083e339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target variable\n",
    "\n",
    "X_train = train_model.drop(['total_cost'], axis=1)\n",
    "y_train = train_model['total_cost']\n",
    "\n",
    "X_test = test_model.drop(['total_cost'], axis=1)\n",
    "y_test = test_model['total_cost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7429b9af-1984-4509-9e00-f69e44dbba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "\n",
    "cat_features = list(X_train.columns[X_train.dtypes==object])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('1hot', OneHotEncoder(handle_unknown= 'ignore', drop = 'first'))\n",
    "])\n",
    "\n",
    "num_features = list(X_train.columns[X_train.dtypes!=object])\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('rob_scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features),\n",
    "])\n",
    "\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('randfor', RandomForestRegressor(random_state=RSEED))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96026830-b2af-449c-b4b0-718a76bfcc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error first Model (train data): 3523182.54\n"
     ]
    }
   ],
   "source": [
    "# cross validate\n",
    "\n",
    "y_train_predicted_cv = cross_val_predict(pipe_rf, X_train, y_train, cv=5)\n",
    "print(\"Mean Absolute Error first Model (train data): {:.2f}\".format(mean_absolute_error(y_train, y_train_predicted_cv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8f25a72-2896-41e1-8ee8-9bf1f4c65a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error first Model (test data): 5031151.30\n"
     ]
    }
   ],
   "source": [
    "# fit and predict\n",
    "\n",
    "y_train_predicted = pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "y_test_predicted = pipe_rf.predict(X_test)\n",
    "print(\"Mean Absolute Error first Model (test data): {:.2f}\".format(mean_absolute_error(y_test, y_test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16416884-ae75-412a-ba3d-7835329590c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c62908c-a8d3-4401-8a36-8d2722f0760c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673d8e3-3a3b-4a8f-955b-d445088cd920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18091a69-3a3c-4f8a-8193-513c3d37d995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
